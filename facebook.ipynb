{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from json import *\n",
    "import numpy as np\n",
    "with open('facebook.json','r')as f:\n",
    "    facebook=f.read()\n",
    "facebook=JSONDecoder().decode(facebook)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9999\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':[0,99999],'UNK':[1,99999]}\n",
    "\n",
    "pos_dict={'PADDING':0,'UNK':1}\n",
    "for i in facebook:\n",
    "    tp=i\n",
    "    for word in tp[0]:\n",
    "        if not word in word_dict:\n",
    "            word_dict[str(word)]=[len(word_dict),1]\n",
    "        else:\n",
    "            word_dict[str(word)][1]+=1\n",
    "    for pos in tp[1]:\n",
    "        if not pos in pos_dict:\n",
    "            pos_dict[str(pos)]=len(pos_dict)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_dict))\n",
    "print(len(pos_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=100\n",
    "count=0\n",
    "all_data=[]\n",
    "all_pos=[]\n",
    "all_label=[]\n",
    "\n",
    "for senter in facebook:\n",
    "    sent=[]\n",
    "    pos=[]\n",
    "    \n",
    "\n",
    "    for word in senter[0]:\n",
    "        if word_dict[str(word)][1]>=1:\n",
    "            sent.append(word_dict[str(word)][0])\n",
    "    for poser in senter[1]:\n",
    "        pos.append(pos_dict[str(poser)])\n",
    "    #labelk.append([float(senter[2][0]),float(senter[2][1])])\n",
    "    if len(sent)<=100:\n",
    "        sent=sent+[0]*(100-len(sent))\n",
    "        pos=pos+[0]*(100-len(pos))\n",
    "        all_data.append(sent)\n",
    "        all_pos.append(pos)\n",
    "        all_label.append([float(senter[2][0]),float(senter[2][1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embdict=dict()\n",
    "plo=0\n",
    "line=0\n",
    "with open('/home/wuch/GoogleNews-vectors-negative300.bin','rb')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in range(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1).decode(\"utf-8\", 'ignore')\n",
    "            if ch ==' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        if len(word) != 0:\n",
    "            tp= np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            if word in word_dict:\n",
    "                embdict[word]=tp.tolist()\n",
    "                if plo%100==0:\n",
    "                    print(plo,line,word)\n",
    "                plo+=1\n",
    "                #print(word,tp)\n",
    "        else:\n",
    "            f.read(binary_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(word_dict))\n",
    "lister=[0]*len(word_dict)\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "for i in embdict.keys():\n",
    "    lister[word_dict[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "\n",
    "    \n",
    "norm=np.random.uniform(-0.1,0.1,(len(word_dict),300))\n",
    "\n",
    "for i in range(len(lister)):\n",
    "    if type(lister[i])==int:\n",
    "        lister[i]=np.reshape(norm[i], 300)\n",
    "lister[0]=np.zeros(300,dtype='float32')\n",
    "lister=np.array(lister,dtype='float32')\n",
    "print(lister.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "l1=list(range(2790))\n",
    "random.shuffle(l1)\n",
    "\n",
    "all_data0=[]\n",
    "all_pos0=[]\n",
    "all_label0=[]\n",
    "\n",
    "for x in l1:\n",
    "    all_pos0.append(all_pos[x])\n",
    "    all_label0.append(all_label[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count=0\n",
    "all_data000=[]\n",
    "for x in l1:\n",
    "    \n",
    "    count+=1\n",
    "    print(count)    \n",
    "    tt1=[]       \n",
    "    for xx in all_data[x]:\n",
    "        tp=[0]*xx+[1]+[0]*(8589-xx)\n",
    "        tt1.append(tp)\n",
    "    all_data000.append(tt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in l1:\n",
    "    all_data0.append(all_data[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for fold in range(10):\n",
    "    all_index=range(len(all_data0))\n",
    "    indexer=random.sample(all_index,int(len(all_data0)*0.9))\n",
    "    \n",
    "    labeled_ratio=0.2\n",
    "    zero_mask=[[1]]*(int(len(all_data0)*0.9)*labeled_ratio)+[[0]]*(int(len(all_data0)*0.9)*(1-labeled_ratio))\n",
    "    \n",
    "    test_index=list(set(all_index)-set(indexer))\n",
    "    \n",
    "    train_item=np.array(all_data0,dtype='int32')[indexer]\n",
    "    train_pos=np.array(all_pos0,dtype='int32')[indexer]\n",
    "    train_label=np.array(all_label0,dtype='float32')[indexer]\n",
    "    \n",
    "    test_item=np.array(all_data0,dtype='int32')[test_index]\n",
    "    test_pos=np.array(all_pos0,dtype='int32')[test_index]\n",
    "    test_label=np.array(all_label0,dtype='float32')[test_index]\n",
    "    from keras.layers import GRU,Bidirectional\n",
    "    import keras.backend as K\n",
    "    from keras.models import Sequential,Model\n",
    "    from keras.layers import Dense, Dropout, Input,Activation,wrappers,Merge,merge,core,advanced_activations,MaxPooling1D,GlobalAveragePooling1D\n",
    "    from keras.layers import LSTM,Conv1D,GlobalMaxPooling1D,SimpleRNN\n",
    "    from keras.layers import Masking, Embedding,Flatten,RepeatVector,Permute,TimeDistributed\n",
    "    from keras.regularizers import l2\n",
    "    from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "    import tensorflow as tf\n",
    "    import random\n",
    "    sequenceLength =100\n",
    "    fmin=0\n",
    "    x_train = Input(batch_shape=(None,sequenceLength),dtype='int32')\n",
    "    x_train2 = Input(batch_shape=(None,sequenceLength),dtype='int32')\n",
    "    y_train = Input(batch_shape=(None,3),dtype='float32')\n",
    "    \n",
    "    \n",
    "    x_test = Input(batch_shape=(None,sequenceLength),dtype='int32')\n",
    "    x_test2 = Input(batch_shape=(None,sequenceLength),dtype='int32')\n",
    "    y_test = Input(batch_shape=(None,3),dtype='float32')\n",
    "    \n",
    "    emb_drop=0.5\n",
    "    pos_emb_drop=0.5\n",
    "    \n",
    "    global_drop=0.5\n",
    "    \n",
    "    hid1=100\n",
    "    emb = Embedding(len(word_dict), 300,weights=[lister],input_length=sequenceLength,mask_zero=False)(x_train)\n",
    "    drop_out_emb = Dropout(emb_drop)(emb)\n",
    "    \n",
    "    pos_emb = Embedding(47, 50,input_length=sequenceLength,mask_zero=False,dropout=pos_emb_drop)(x_train2)\n",
    "    drop_out_pos_emb = Dropout(pos_emb_drop)(pos_emb)\n",
    "    \n",
    "    merged = merge([drop_out_emb, drop_out_pos_emb], mode='concat',concat_axis=2)\n",
    "    \n",
    "\n",
    "    \n",
    "    flat_beta=Bidirectional(LSTM(hid1,return_sequences=True))(merged)\n",
    "    flat_beta2=Bidirectional(LSTM(hid1,return_sequences=False))(flat_beta)\n",
    "    flat_betad = Dropout(global_drop)(flat_beta2)\n",
    "    \n",
    "    outer=Dense(3)(flat_betad)\n",
    "\n",
    " \n",
    "    h =LSTM(100,batch_input_shape=(batch_size,sequenceLength, 350),return_sequences=False,dropout_W=0.5,dropout_U=0.5)(merged)\n",
    "\n",
    "    h_merge=merge([h, outer], mode='concat',concat_axis=-1)\n",
    "    z_mean = Dense(latent_dim)(h_merge)\n",
    "    z_log_sigma = Dense(latent_dim)(h_merge)\n",
    "    \n",
    "    \n",
    "    def sampling(args):\n",
    "        z_mean, z_log_sigma = args\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                                  stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_sigma / 2) * epsilon\n",
    "    \n",
    "    z = Lambda(sampling)([z_mean, z_log_sigma])\n",
    "    \n",
    "    \n",
    "    decoder = RecurrentSequential(decode=True, output_length=sequenceLength,unroll=False, stateful=False)\n",
    "    decoder.add(Dropout(0.5))\n",
    "    decoder.add(LSTMDecoderCell(output_dim=100, hidden_dim=100))\n",
    "    h_decoded = decoder(z)\n",
    "    \n",
    "    x_decoded_mean = TimeDistributed(Dense(len(word_dict),activation='softmax'))(h_decoded)\n",
    "\n",
    "    model = Model(input=[x_train2,xx,x_train3], output=[outer,x_decoded_mean])\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "\n",
    "        xent_loss = K.mean(metrics.categorical_crossentropy(x, x_decoded_mean))\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "        return xent_loss+kl_loss\n",
    "\n",
    "    def myloss(y_true, y_pred):\n",
    "        return K.mean(x_train3*K.abs(y_pred - y_true), axis=-1)\n",
    "    \n",
    "\n",
    "    model.compile(loss=[myloss,vae_loss],loss_weights=[0.5*(1/(labeled_ratio)), 1.],\n",
    "                  optimizer='rmsprop',metrics=['mae'])\n",
    "    early=EarlyStopping(monitor='val_loss',patience=10)\n",
    "\n",
    "    model.fit([train_pos,train_item0,train_mask],[train_label,train_item0], batch_size=batch_size, nb_epoch=50,shuffle=True,validation_split=0.1,callbacks=[early])\n",
    "        \n",
    "    classes=model.predict([test_item,test_pos], batch_size=64)\n",
    "    \n",
    "    from scipy.stats.stats import pearsonr\n",
    "    mae=np.mean(np.abs(classes-test_label),axis=0)\n",
    "    rmse=np.sqrt(np.mean(np.square(classes-test_label),axis=0))\n",
    "    pcc1=pearsonr(classes[:,0],test_label[:,0])[0]\n",
    "    pcc2=pearsonr(classes[:,1],test_label[:,1])[0]\n",
    "    pcc3=pearsonr(classes[:,2],test_label[:,2])[0]\n",
    "\n",
    "    print('mae:',mae[0],mae[1],mae[2])\n",
    "    print('pcc:',pcc1,pcc2,pcc3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
