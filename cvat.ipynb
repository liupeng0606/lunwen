{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from json import *\n",
    "import numpy as np\n",
    "with open('cva_data.json','r')as f:\n",
    "    cva_data=f.read()\n",
    "cva_data=JSONDecoder().decode(cva_data)\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9999\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi=[x[2:4] for x in cva_data]\n",
    "inter=[0]*9\n",
    "for i in fi:\n",
    "    inter[int(np.round(i[0])-1)]+=1 \n",
    "#fi=np.array([x[2] for x in facebook],dtype='float32')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(np.arange(9),inter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':[0,999999],'UNK':[1,99999]}\n",
    "\n",
    "pos_dict={'PADDING':0,'UNK':1}\n",
    "for i in cva_data:\n",
    "    tp=i\n",
    "    for word in tp[0]:\n",
    "        if not word in word_dict:\n",
    "            word_dict[str(word)]=[len(word_dict),1]\n",
    "        else:\n",
    "            word_dict[str(word)][1]+=1\n",
    "    for pos in tp[1]:\n",
    "        if not pos in pos_dict:\n",
    "            pos_dict[str(pos)]=len(pos_dict)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trp=sorted(word_dict.items(), key=lambda d:d[1][1], reverse=True)\n",
    "word_dict={}\n",
    "co=0\n",
    "\n",
    "for i in trp:\n",
    "    if i[1][1]>=2:\n",
    "        word_dict[i[0]]=[co,i[1][1]]\n",
    "        #print(co)\n",
    "    co+=1\n",
    "print(len(word_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embdict={}\n",
    "plo=0\n",
    "import  pickle\n",
    "with open('/home/wuch/chinese_word_new.bin','r')as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    while True:\n",
    "        ch =  f.readline()\n",
    "        if not ch:\n",
    "            break\n",
    "        k=ch.find(\" \")\n",
    "        word=ch[0:k]\n",
    "        emb=ch[k+1:-2].split()\n",
    "        if word in word_dict and len(emb)==300:\n",
    "            for i in range(len(emb)):\n",
    "                emb[i]=float(emb[i])\n",
    "            embdict[str(word)]=emb\n",
    "        plo+=1\n",
    "        if plo%10000==1:\n",
    "            print(plo,len(embdict),word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import cholesky\n",
    "print(len(embdict),len(word_dict))\n",
    "print(len(word_dict))\n",
    "lister=[0]*len(word_dict)#4600\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "\n",
    "for i in embdict.keys():\n",
    "    lister[word_dict[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    cand.append(lister[word_dict[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "#R = cholesky(Sigma)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(lister)):\n",
    "    if type(lister[i])==int:\n",
    "        lister[i]=np.reshape(norm, 300)\n",
    "lister[0]=np.zeros(300,dtype='float32')\n",
    "lister=np.array(lister,dtype='float32')\n",
    "print(lister.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word=[]\n",
    "all_word_pos=[]\n",
    "all_word_label=[]\n",
    "\n",
    "maxlen=135\n",
    "count=0\n",
    "for i in cva_data:\n",
    "    tp=i\n",
    "    sent=[]\n",
    "    pos=[]\n",
    "    for word in tp[0]:\n",
    "        if word in word_dict:\n",
    "            sent.append(word_dict[str(word)][0])\n",
    "        else:\n",
    "            sent.append(1)\n",
    "    for po in tp[1]:\n",
    "        pos.append(pos_dict[str(po)])   \n",
    "    if len(sent)<=maxlen:\n",
    "        count+=1\n",
    "        sent=sent+[0]*(maxlen-len(sent))\n",
    "        pos=pos+[0]*(maxlen-len(pos))\n",
    "        all_word.append(sent)\n",
    "        all_word_pos.append(pos)\n",
    "        all_word_label.append([tp[2],tp[3]])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "all_data_sparse=[]\n",
    "l1=list(range(2000))\n",
    "random.shuffle(l1)\n",
    "all_word0=[]\n",
    "all_pos0=[]\n",
    "all_word_label0=[]\n",
    "count=0\n",
    "for x in l1:\n",
    "\n",
    "    all_word0.append(all_word[x])\n",
    "    all_pos0.append(all_word_pos[x])\n",
    "    all_word_label0.append(all_word_label[x])\n",
    "    tt1=[]\n",
    "    for xx in all_word[x]:\n",
    "        tp=[0]*xx+[1]+[0]*(5302-xx)\n",
    "        tt1.append(tp)\n",
    "    all_data_sparse.append(tt1)\n",
    "    count+=1\n",
    "    print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from recurrentshop import LSTMCell, RecurrentSequential\n",
    "from seq2seq.cells import *\n",
    "from seq2seq.models import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, TimeDistributed, Bidirectional, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#semi-supervised\n",
    "result=[]\n",
    "for fold in range(10):\n",
    "    labeled_ratio=0.2\n",
    "    zero_mask=[[1]]*(1800*labeled_ratio)+[[0]]*(1800*(1-labeled_ratio))\n",
    "    import random\n",
    "    all_index=range(len(all_data_sparse))\n",
    "    indexer=random.sample(all_index,1800)\n",
    "    test_index=list(set(all_index)-set(indexer))\n",
    "    train_item=np.array(all_word0,dtype='int32')[indexer]\n",
    "    train_item0=np.array(all_data_sparse,dtype='int32')[indexer]\n",
    "    train_pos=np.array(all_pos0,dtype='int32')[indexer]\n",
    "    train_label=np.array(all_word_label0,dtype='float32')[indexer]\n",
    "    train_mask=np.array(zero_mask,dtype='float32')\n",
    "    \n",
    "    test_item0=np.array(all_data_sparse,dtype='int32')[test_index]\n",
    "    test_item=np.array(all_word0,dtype='int32')[test_index]\n",
    "    test_pos=np.array(all_pos0,dtype='int32')[test_index]\n",
    "    test_label=np.array(all_word_label0,dtype='float32')[test_index]\n",
    "    test_mask=np.array([[1]]*200,dtype='float32')\n",
    "    print('done')\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.stats import norm\n",
    "    import tensorflow as tf  \n",
    "    from keras.layers import Input, Dense, Lambda, Layer\n",
    "    from keras.models import Model\n",
    "    from keras import backend as K\n",
    "    from keras import metrics\n",
    "    from keras.datasets import mnist\n",
    "    from keras.layers import GRU,Bidirectional,Multiply\n",
    "    from keras.models import Sequential,Model\n",
    "    from keras.layers import Dense, Dropout, Input,Activation,wrappers,Merge,merge,core,advanced_activations\n",
    "    from keras.layers import LSTM,Conv1D,GlobalMaxPooling1D,SimpleRNN,MaxPooling1D\n",
    "    from keras.layers import Masking, Embedding,Flatten,RepeatVector,Permute,TimeDistributed\n",
    "    from keras.regularizers import l2\n",
    "    from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "    sequenceLength =135\n",
    "    fmin=0\n",
    "    #x_train = Input(batch_shape=(None,sequenceLength),dtype='int32')\n",
    "    x_train2 = Input(batch_shape=(None,sequenceLength),dtype='int32')\n",
    "    x_train3 = Input(batch_shape=(None,1),dtype='float32')\n",
    "    #x_train3 = Input(batch_shape=(None,sequenceLength,300),dtype='float32')\n",
    "    y_train = Input(batch_shape=(None,2),dtype='float32')\n",
    "    batch_size=50\n",
    "    latent_dim = 100\n",
    "    epsilon_std = 1.0\n",
    "    emb_drop=0.5\n",
    "    pos_emb_drop=0.5\n",
    "    \n",
    "    global_drop=0.5\n",
    "    \n",
    "    hid1=100\n",
    "    xx = Input(batch_shape=(None, sequenceLength,5303),dtype='int32')\n",
    "\n",
    "    def argmax1(z):\n",
    "        return K.argmax(z)\n",
    "    \n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    zx = Lambda(argmax1, output_shape=(None, sequenceLength))(xx)\n",
    "    \n",
    "    emb = Embedding(5303, 300,weights=[lister],input_length=sequenceLength,mask_zero=False)(zx)\n",
    "    drop_out_emb = Dropout(emb_drop)(emb)\n",
    "    \n",
    "    pos_emb = Embedding(52, 50,input_length=sequenceLength,mask_zero=False,dropout=pos_emb_drop)(x_train2)\n",
    "    drop_out_pos_emb = Dropout(pos_emb_drop)(pos_emb)\n",
    "    \n",
    "    merged = merge([drop_out_emb, drop_out_pos_emb], mode='concat',concat_axis=2)\n",
    "    #merged2 = merge([merged,x_train3], mode='concat',concat_axis=2)\n",
    "\n",
    "\n",
    "    flat_beta=Bidirectional(LSTM(hid1,return_sequences=True))(merged)\n",
    "    flat_beta2=Bidirectional(LSTM(hid1,return_sequences=False))(flat_beta)\n",
    "    flat_betad = Dropout(global_drop)(flat_beta2)\n",
    "\n",
    "    outer=Dense(2)(flat_betad)\n",
    "\n",
    "    \n",
    "    h =LSTM(100,batch_input_shape=(batch_size,sequenceLength, 350),return_sequences=False,dropout_W=0.5,dropout_U=0.5)(merged)\n",
    "\n",
    "    h_merge=merge([h, outer], mode='concat',concat_axis=-1)\n",
    "    z_mean = Dense(latent_dim)(h_merge)\n",
    "    z_log_sigma = Dense(latent_dim)(h_merge)\n",
    "    \n",
    "    \n",
    "    def sampling(args):\n",
    "        z_mean, z_log_sigma = args\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
    "                                  stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_sigma / 2) * epsilon\n",
    "    \n",
    "    z = Lambda(sampling)([z_mean, z_log_sigma])\n",
    "    \n",
    "    \n",
    "    decoder = RecurrentSequential(decode=True, output_length=sequenceLength,unroll=False, stateful=False)\n",
    "    decoder.add(Dropout(0.5))\n",
    "    decoder.add(LSTMDecoderCell(output_dim=100, hidden_dim=100))\n",
    "    h_decoded = decoder(z)\n",
    "    \n",
    "    x_decoded_mean = TimeDistributed(Dense(5303,activation='softmax'))(h_decoded)\n",
    "\n",
    "    model = Model(input=[x_train2,xx,x_train3], output=[outer,x_decoded_mean])\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "\n",
    "        xent_loss = K.mean(metrics.categorical_crossentropy(x, x_decoded_mean))\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "        return xent_loss+kl_loss\n",
    "\n",
    "    def myloss(y_true, y_pred):\n",
    "        return K.mean(x_train3*K.abs(y_pred - y_true), axis=-1)\n",
    "    \n",
    "\n",
    "    model.compile(loss=[myloss,vae_loss],loss_weights=[0.5*(1/(labeled_ratio)), 1.],\n",
    "                  optimizer='rmsprop',metrics=['mae'])\n",
    "    early=EarlyStopping(monitor='val_loss',patience=10)\n",
    "\n",
    "    model.fit([train_pos,train_item0,train_mask],[train_label,train_item0], batch_size=batch_size, nb_epoch=50,shuffle=True,validation_split=0.1,callbacks=[early])\n",
    "    model2 = Model(input=[x_train2,xx,x_train3], output=[outer])\n",
    "    classes=model2.predict([test_pos,test_item0,test_mask], batch_size=50)\n",
    "\n",
    "    \n",
    "    from scipy.stats.stats import pearsonr\n",
    "    mae=np.mean(np.abs(classes-test_label),axis=0)\n",
    "    rmse=np.sqrt(np.mean(np.square(classes-test_label),axis=0))\n",
    "    pcc1=pearsonr(classes[:,0],test_label[:,0])[0]\n",
    "    pcc2=pearsonr(classes[:,1],test_label[:,1])[0]\n",
    "    result.append([rmse[0],rmse[1],mae[0],mae[1],pcc1,pcc2])\n",
    "    print('\\n=======Report=======')\n",
    "    print('rmse:',rmse[0],rmse[1])\n",
    "    print('mae:',mae[0],mae[1])\n",
    "    print('pcc:',pcc1,pcc2)\n",
    "result=np.array(result,dtype='float32')\n",
    "print('\\n=======ave Result=======')\n",
    "print(np.mean(result,axis=0))\n",
    "print('\\n=======ave Result=======')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
